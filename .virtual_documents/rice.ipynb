import kagglehub

# Download latest version
path = kagglehub.dataset_download("muratkokludataset/rice-image-dataset")

print("Path to dataset files:", path)


import os 
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns


import torch
from torch import nn


from pathlib import Path

path = Path("/home/mohsinkhan/.cache/kagglehub/datasets/muratkokludataset/rice-image-dataset/versions/1/Rice_Image_Dataset")
os.listdir(path)


def walk_throght_dir(target_dir):
    for root, dirs, files in os.walk(target_dir):
        print(f"current directory : {root}")
        print(f"Subdirectory : {len(dirs)}")
        print(f"file {len(files)}")
walk_throght_dir(path)


path


import random
import PIL

def view_image(path):
    # image path list
    image_path_list = list(path.rglob("*.jpg"))
    
    # chose a random image
    random_image = random.choice(image_path_list) 
    
    # class name
    image_class_name = random_image.parent.stem
    
    img = PIL.Image.open(random_image)
    
    # let's see image
    print(f"image path {random_image}")
    print(f"image hight {img.height}")
    print(f"image width {img.width}")
    print(f"image class {image_class_name}\n")
    return img, image_class_name

img, image_class_name = view_image(path)


# convert image into an array and then visualize it 
image_as_array = np.asarray(img)
plt.imshow(image_as_array)
plt.title(f"image of shape {image_as_array.shape} | image class {image_class_name} \n")
plt.axis(False)
plt.show()


from sklearn.model_selection import train_test_split

image_paths = list(path.rglob("*.jpg"))
labels = [p.parent.stem for p in image_paths]

train_paths, test_paths = train_test_split(
    image_paths,
    test_size=0.2,
    stratify=labels,
    random_state=42
)



len(train_paths), len(test_paths)



def view_image(path):
    # image path list
    # image_path_list = list(path.rglob("*.jpg"))
    
    # chose a random image
    random_image = random.choice(path) 
    
    # class name
    image_class_name = random_image.parent.stem
    
    img = PIL.Image.open(random_image)
    
    # let's see image
    print(f"image path {random_image}")
    print(f"image hight {img.height}")
    print(f"image width {img.width}")
    print(f"image class {image_class_name}\n")
    return img, image_class_name

img, image_class_name = view_image(train_paths)
img


# we can't use imagefolder here since it needs a clear structure of directory, and we don't have that,t  we are using paths
# let's create labels
classes =  sorted(set(labels))
# give them their index as label
class_to_idx  = {cls:idx for idx, cls in enumerate(classes)}
class_to_idx


# create a castum dateset
from torch.utils.data import Dataset

class RiceDataset(Dataset):
    def __init__(self, image_path, class_to_idx, transform=None):
        self.image_path =  image_path
        self.class_to_idx = class_to_idx
        self.transform = transform

    def __len__(self):
        return len(self.image_path)

    def __getitem__(self, idx):
        # loading  image
        img_path = self.image_path[idx]
        image = PIL.Image.open(img_path).convert("RGB")

        label_name = img_path.parent.stem
        label =  self.class_to_idx[label_name]

        if self.transform:
            image =  self.transform(image)
        return image, label



import cv2
import numpy as np
from PIL import Image

class CropRice:
    def __call__(self, img):
        img = np.array(img)
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)
        coords = cv2.findNonZero(thresh)

        x, y, w, h = cv2.boundingRect(coords)
        cropped = img[y:y+h, x:x+w]

        return Image.fromarray(cropped)



# defining transforms 

from torchvision import transforms
train_transform = transforms.Compose([
    CropRice(),                        
    transforms.Resize((224,224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

test_transform = transforms.Compose([
    CropRice(),
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])



train_data = RiceDataset(train_paths, class_to_idx, transform=train_transform)
test_data = RiceDataset(test_paths, class_to_idx, transform=test_transform)


train_data.class_to_idx


len(train_data), len(test_data)


def display_random_images(dataset:torch.utils.dataset.Dataset,
                         classes: List[str]=None,
                         n:int=10, 
                         display_shape:bool =True,
                         seed:int =None):
    # adjust display if n is too high:
    if n > 10:
        n=10
        print(f"n is adjusted to 10......")

    # set the random seed
    if seed:
        random.seed(seed)

    # get random sample index
    random_sample_idx = random.sample(range(len(dataset)), k=n)
    
    # setup plot
    plt.figure(figsize=(16,8))

    # loop through the sample and display random samples
    for i, targ_sample in enumerate(random_sample_idx):
        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]

        targ_image_adjust = targ_image.permute(1,2,0)
        plt.subplot(1,n,i+1)
        plt.imshow(targ_image_adjust)
        plt.axis('off')
        if classes:
            title = f"class : {classes[targ_label]}"
            if display_shape:
                title = title + f"\nshape : {targ_image_adjust.shape}"
        plt.title(title)


display_random_images(train_data,
                     classes, 
                     n=5)


# convert to datloader  of with batch_size=32
# import dataloader
from torch.utils.data import DataLoader

# define batch size
BATCH_SIZE=32 

train_dataloader = DataLoader(train_data,
                             batch_size=BATCH_SIZE,
                             num_workers=0,
                             shuffle=True)


test_dataloader = DataLoader(test_data,
                             batch_size=BATCH_SIZE,
                             num_workers=0)


print(f"Number of training  batch {len(train_dataloader)} of batch size {BATCH_SIZE}")
print(f"Number of testing batch {len(test_dataloader)} of batch size {BATCH_SIZE}")



images, labels = next(iter(train_dataloader))

print(f"shape eith batch size : {images.shape} label : {labels.shape} ")
img = images[0]      
label = labels[0]     

# convert tensor (C,H,W) â†’ (H,W,C)
img = img.permute(1, 2, 0)

plt.imshow(img)
plt.title(f"Label: {label} | Shape: {img.shape}")
plt.axis("off")
plt.show()


# set device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"


# let's build a model 

import torch.nn as nn

class RiceClassifier(nn.Module):
    def __init__(self, in_channels=3, num_classes=5):
        super().__init__()

        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),   # 112Ã—112

            # Block 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),   # 56Ã—56

            # Block 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),   # 28Ã—28
        )

        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),  # ðŸ”¥ removes hardcoding
            nn.Flatten(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)


model_0 = RiceClassifier().to(device)
model_0


from torchinfo import summary
summary(model_0, input_size=images.shape)


# now wehave model, we need to define a loss function, optimizer, and accuracy
from sklearn.metrics import accuracy_score

loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(params=model_0.parameters(),
                            lr=0.01)


def train_steps(model: nn.Module,
                loss_fn: nn.Module,
                optimizer: torch.optim.Optimizer,
                dataloader: torch.utils.data.DataLoader,
                device=device):

    train_loss, train_acc = 0.0, 0.0

    # put model in training mode ONCE
    model.train()

    for X, y in dataloader:
        X, y = X.to(device), y.to(device)

        # Forward pass
        y_logits = model(X)
        loss = loss_fn(y_logits, y)

        # Predictions
        y_pred = torch.argmax(y_logits, dim=1)

        # Accuracy (torch-based)
        acc = (y_pred == y).sum().item() / y.size(0)

        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Accumulate metrics
        train_loss += loss.item()
        train_acc += acc

    train_loss /= len(dataloader)
    train_acc /= len(dataloader)

    return train_loss, train_acc


def test_steps(model: nn.Module,
               loss_fn: nn.Module,
               dataloader: torch.utils.data.DataLoader,
               device=device):
    
    test_loss, test_acc = 0.0, 0.0
    model.eval()

    with torch.inference_mode():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)

            # Forward pass
            y_logits = model(X)
            loss = loss_fn(y_logits, y)

            # Predictions
            y_pred = torch.argmax(y_logits, dim=1)

            # Accuracy
            acc = (y_pred == y).sum().item() / y.size(0)

            test_loss += loss.item()
            test_acc += acc

    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

    return test_loss, test_acc



def train_model(model: nn.Module,
                loss_fn: nn.Module,
                optimizer: torch.optim.Optimizer,
                train_dataloader: torch.utils.data.DataLoader,
                test_dataloader: torch.utils.data.DataLoader,
                epochs: int,
                device=device,
                patience: int = 2):

    best_test_loss = float("inf")
    best_test_acc = 0.0
    epochs_no_improve = 0

    for epoch in range(1, epochs + 1):

        train_loss, train_acc = train_steps(
            model=model,
            loss_fn=loss_fn,
            optimizer=optimizer,
            dataloader=train_dataloader,
            device=device
        )

        test_loss, test_acc = test_steps(
            model=model,
            loss_fn=loss_fn,
            dataloader=test_dataloader,
            device=device
        )

        print(f"Epoch: {epoch}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Test  Loss: {test_loss:.4f} | Test  Acc: {test_acc:.4f}")
        print("-" * 50)

        if test_loss < best_test_loss or test_acc > best_test_acc:
            best_test_loss = min(best_test_loss, test_loss)
            best_test_acc = max(best_test_acc, test_acc)
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            print(f"No improvement for {epochs_no_improve} epoch(s)")

        if epochs_no_improve >= patience:
            print("Early stopping triggered")
            break



train_model(model_0,
           loss_fn=loss_fn,
           optimizer=optimizer,
           train_dataloader=train_dataloader,
           test_dataloader=test_dataloader,
           epochs=15,
           )












